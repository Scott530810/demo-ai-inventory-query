"""
FastAPI Server for Remote Ambulance Inventory Queries
å¯å¾é ç«¯ Windows 11 ç­†é›»é€£ç·šæŸ¥è©¢çš„ API æœå‹™å™¨

é‹è¡Œæ–¹å¼:
    uvicorn server.api_server:app --host 0.0.0.0 --port 8000

é ç«¯è¨ªå•:
    http://SPARK_IP:8000/docs
"""

from fastapi import FastAPI, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, RedirectResponse
from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any
import sys
import os
import time
from pathlib import Path

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from ambulance_inventory import __version__ as app_version
from ambulance_inventory.config import DatabaseConfig, OllamaConfig, RagConfig
from ambulance_inventory.database import DatabaseClient
from ambulance_inventory.ollama_client import OllamaClient
from ambulance_inventory.query_engine import QueryEngine
from ambulance_inventory.rag.retriever import RagRetriever
from ambulance_inventory.utils.logger import get_logger

logger = get_logger(__name__)

# Initialize FastAPI app
app = FastAPI(
    title="Ambulance Inventory Query API",
    description="è‡ªç„¶èªè¨€æŸ¥è©¢æ•‘è­·è»Šè¨­å‚™åº«å­˜ç³»çµ± - é ç«¯ API ç‰ˆæœ¬",
    version=app_version,
    docs_url="/docs",
    redoc_url="/redoc"
)

# CORS configuration for remote access from Windows 11
cors_origins_raw = os.getenv("CORS_ALLOW_ORIGINS", "*")
cors_origins = [origin.strip() for origin in cors_origins_raw.split(",") if origin.strip()]
if not cors_origins:
    cors_origins = ["*"]

cors_credentials_env = os.getenv("CORS_ALLOW_CREDENTIALS")
if cors_credentials_env is None:
    cors_allow_credentials = cors_origins != ["*"]
else:
    cors_allow_credentials = cors_credentials_env.strip().lower() in {"1", "true", "yes"}

app.add_middleware(
    CORSMiddleware,
    allow_origins=cors_origins,
    allow_credentials=cors_allow_credentials,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Static files for web UI
web_dir = Path(__file__).parent.parent / "web"
if web_dir.exists():
    app.mount("/static", StaticFiles(directory=str(web_dir)), name="static")

# Global clients (initialized on startup)
db_client: Optional[DatabaseClient] = None
ollama_client: Optional[OllamaClient] = None
query_engine: Optional[QueryEngine] = None
rag_retriever: Optional[RagRetriever] = None


# Pydantic models
class QueryRequest(BaseModel):
    """æŸ¥è©¢è«‹æ±‚"""
    question: str = Field(..., description="è‡ªç„¶èªè¨€å•é¡Œ", min_length=1)
    model: Optional[str] = Field(None, description="ä½¿ç”¨çš„æ¨¡å‹ï¼ˆå¯é¸ï¼Œä¸æŒ‡å®šå‰‡ä½¿ç”¨ç•¶å‰æ¨¡å‹ï¼‰")
    use_llm_answer: bool = Field(True, description="æ˜¯å¦ä½¿ç”¨ LLM ç”Ÿæˆå›ç­”ï¼ˆFalse å‰‡åªç”¨ç¨‹å¼åŒ–æ ¼å¼ï¼Œæ›´å¿«ï¼‰")
    rag_mode: Optional[str] = Field("sql_only", description="RAG æ¨¡å¼ï¼šsql_only / rag_only / hybrid")
    rag_top_k: Optional[int] = Field(None, description="RAG å–ç”¨ç‰‡æ®µæ•¸é‡ï¼ˆå¯é¸ï¼‰")

    class Config:
        json_schema_extra = {
            "example": {
                "question": "è«‹åˆ—å‡ºæ‰€æœ‰æœ‰åº«å­˜çš„AEDé™¤é¡«å™¨ï¼ŒåŒ…å«å“ç‰Œã€å‹è™Ÿå’Œåº«å­˜æ•¸é‡",
                "model": "llama3:70b",
                "use_llm_answer": True
            }
        }


class ModelsResponse(BaseModel):
    """æ¨¡å‹åˆ—è¡¨å›æ‡‰"""
    models: List[str] = Field(..., description="å¯ç”¨æ¨¡å‹åˆ—è¡¨")
    current: str = Field(..., description="ç•¶å‰ä½¿ç”¨çš„æ¨¡å‹")


class ModelSelectRequest(BaseModel):
    """æ¨¡å‹é¸æ“‡è«‹æ±‚"""
    model: str = Field(..., description="è¦ä½¿ç”¨çš„æ¨¡å‹åç¨±")


class TimingInfo(BaseModel):
    """è¨ˆæ™‚è³‡è¨Š"""
    sql_generation: Optional[float] = Field(None, description="SQL ç”Ÿæˆè€—æ™‚ï¼ˆç§’ï¼‰")
    query_execution: Optional[float] = Field(None, description="æŸ¥è©¢åŸ·è¡Œè€—æ™‚ï¼ˆç§’ï¼‰")
    formatting: Optional[float] = Field(None, description="æ ¼å¼åŒ–è€—æ™‚ï¼ˆç§’ï¼‰")
    llm_response: Optional[float] = Field(None, description="LLM å›ç­”ç”Ÿæˆè€—æ™‚ï¼ˆç§’ï¼‰")
    total: Optional[float] = Field(None, description="ç¸½è€—æ™‚ï¼ˆç§’ï¼‰")


class QueryResponse(BaseModel):
    """æŸ¥è©¢å›æ‡‰"""
    question: str = Field(..., description="åŸå§‹å•é¡Œ")
    sql: str = Field(..., description="ç”Ÿæˆçš„ SQL æŸ¥è©¢")
    answer: str = Field(..., description="AI å›ç­”ï¼ˆLLM ç”Ÿæˆï¼‰")
    answer_formatted: Optional[str] = Field(None, description="ç¨‹å¼åŒ–æ ¼å¼å›ç­”ï¼ˆç´”æ–‡å­—è¡¨æ ¼ï¼‰")
    answer_html: Optional[str] = Field(None, description="HTML è¡¨æ ¼æ ¼å¼ï¼ˆå®Œç¾å°é½Šï¼Œæ¨è–¦ç”¨æ–¼ Webï¼‰")
    results: Optional[List[Dict[str, Any]]] = Field(None, description="åŸå§‹æŸ¥è©¢çµæœ")
    result_count: Optional[int] = Field(None, description="çµæœç­†æ•¸")
    rag_context: Optional[List[Dict[str, Any]]] = Field(None, description="RAG æª¢ç´¢çµæœï¼ˆç‰‡æ®µï¼‰")
    rag_mode: Optional[str] = Field(None, description="RAG æ¨¡å¼")
    model_used: Optional[str] = Field(None, description="å¯¦éš›ä½¿ç”¨çš„æ¨¡å‹åç¨±")
    use_llm_answer: Optional[bool] = Field(None, description="æ˜¯å¦ä½¿ç”¨ LLM ç”Ÿæˆå›ç­”ï¼ˆå¯¦éš›åŸ·è¡Œçš„æ¨¡å¼ï¼‰")
    elapsed_time: Optional[float] = Field(None, description="ç¸½è€—æ™‚ï¼ˆç§’ï¼‰")
    timing: Optional[TimingInfo] = Field(None, description="è©³ç´°è¨ˆæ™‚è³‡è¨Š")
    success: bool = Field(..., description="æŸ¥è©¢æ˜¯å¦æˆåŠŸ")
    error: Optional[str] = Field(None, description="éŒ¯èª¤è¨Šæ¯ï¼ˆå¦‚æœæœ‰ï¼‰")


class HealthResponse(BaseModel):
    """å¥åº·æª¢æŸ¥å›æ‡‰"""
    status: str
    database: bool
    ollama: bool
    model: str
    version: str


class TableInfo(BaseModel):
    """è³‡æ–™è¡¨è³‡è¨Š"""
    table_name: str
    columns: List[Dict[str, str]]


@app.on_event("startup")
async def startup_event():
    """æœå‹™å™¨å•Ÿå‹•æ™‚åˆå§‹åŒ–"""
    global db_client, ollama_client, query_engine, rag_retriever

    try:
        logger.info("ğŸš€ Initializing API server...")

        # Initialize database client
        db_config = DatabaseConfig.from_env()
        db_client = DatabaseClient(db_config)
        logger.info("âœ… Database client initialized")

        # Initialize Ollama client
        ollama_config = OllamaConfig.from_env()
        ollama_client = OllamaClient(ollama_config)
        logger.info(f"âœ… Ollama client initialized (model: {ollama_config.model})")

        # Initialize query engine
        query_engine = QueryEngine(db_client, ollama_client)
        logger.info("âœ… Query engine initialized")

        # Initialize RAG retriever
        rag_config = RagConfig.from_env()
        rag_retriever = RagRetriever(db_client, ollama_client, rag_config)
        logger.info("âœ… RAG retriever initialized")

        logger.info("ğŸ‰ API server ready for remote connections!")

    except Exception as e:
        logger.error(f"âŒ Failed to initialize server: {e}")
        raise


@app.on_event("shutdown")
async def shutdown_event():
    """æœå‹™å™¨é—œé–‰æ™‚æ¸…ç†"""
    global db_client

    if db_client:
        db_client.close()
        logger.info("Database connection closed")


@app.get("/", tags=["General"])
async def root():
    """æ ¹ç«¯é» - é‡å®šå‘åˆ° Web UI"""
    return RedirectResponse(url="/web")


@app.get("/web", tags=["General"])
async def web_ui():
    """Web UI ä»‹é¢"""
    web_file = Path(__file__).parent.parent / "web" / "index.html"
    if web_file.exists():
        return FileResponse(str(web_file), media_type="text/html")
    return {"error": "Web UI not found", "path": str(web_file)}


@app.get("/api", tags=["General"])
async def api_info():
    """API è³‡è¨Š"""
    model_name = ollama_client.config.model if ollama_client else "unknown"
    return {
        "message": "Ambulance Inventory Query API",
        "version": app_version,
        "model": model_name,
        "docs": "/docs",
        "health": "/health",
        "web_ui": "/web"
    }


@app.get("/health", response_model=HealthResponse, tags=["General"])
async def health_check():
    """
    å¥åº·æª¢æŸ¥ç«¯é»

    æª¢æŸ¥è³‡æ–™åº«å’Œ Ollama é€£æ¥ç‹€æ…‹
    """
    try:
        # Check database
        db_ok = db_client.test_connection() if db_client else False

        # Check Ollama
        ollama_ok = ollama_client.test_connection() if ollama_client else False

        model_name = ollama_client.config.model if ollama_client else "unknown"

        status = "healthy" if (db_ok and ollama_ok) else "unhealthy"

        return HealthResponse(
            status=status,
            database=db_ok,
            ollama=ollama_ok,
            model=model_name,
            version=app_version
        )
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        raise HTTPException(status_code=503, detail=f"Service unavailable: {str(e)}")


@app.post("/query", response_model=QueryResponse, tags=["Query"])
async def query(request: QueryRequest):
    """
    åŸ·è¡Œè‡ªç„¶èªè¨€æŸ¥è©¢

    æ¥æ”¶è‡ªç„¶èªè¨€å•é¡Œï¼Œç”Ÿæˆ SQLï¼ŒåŸ·è¡ŒæŸ¥è©¢ï¼Œè¿”å›å›ç­”

    Args:
        request: åŒ…å«å•é¡Œçš„æŸ¥è©¢è«‹æ±‚ï¼Œå¯é¸æŒ‡å®šæ¨¡å‹å’Œè¼¸å‡ºæ¨¡å¼

    Returns:
        QueryResponse: åŒ…å« SQLã€ç­”æ¡ˆç­‰è³‡è¨Š
        - answer: LLM ç”Ÿæˆçš„è‡ªç„¶èªè¨€å›ç­”
        - answer_formatted: ç¨‹å¼åŒ–è¡¨æ ¼æ ¼å¼ï¼ˆå¿«é€Ÿä¸€è‡´ï¼‰
        - results: åŸå§‹æŸ¥è©¢çµæœï¼ˆJSONï¼‰
    """
    start_time = time.time()

    if not query_engine:
        raise HTTPException(status_code=503, detail="Query engine not initialized")

    # Check Ollama connection first
    if ollama_client and not ollama_client.test_connection():
        return QueryResponse(
            question=request.question,
            sql="",
            answer="",
            answer_formatted=None,
            answer_html=None,
            results=None,
            result_count=None,
            rag_context=None,
            rag_mode=request.rag_mode,
            model_used=ollama_client.config.model if ollama_client else None,
            use_llm_answer=request.use_llm_answer,
            elapsed_time=round(time.time() - start_time, 2),
            success=False,
            error="Ollama service is not available. Please ensure Ollama is running on the server."
        )

    try:
        # Determine which model to use (from request or default)
        # NOTE: We pass the model as a parameter, NOT modifying global state
        # This ensures thread-safety for concurrent requests
        default_model = ollama_client.config.model if ollama_client else "unknown"
        actual_model_used = default_model

        if request.model and ollama_client:
            available_models = ollama_client.get_available_models()
            if request.model in available_models:
                actual_model_used = request.model
                logger.info(f"ğŸ“ Using requested model: {request.model}")
            else:
                logger.warning(f"âš ï¸ Requested model '{request.model}' not available, using default: {default_model}")

        logger.info(f"ğŸ“ Received query: {request.question} (use_llm_answer={request.use_llm_answer}, model={actual_model_used})")

        # RAG retrieval (optional)
        rag_context = None
        rag_mode = request.rag_mode or "sql_only"
        if rag_mode in {"rag_only", "hybrid"}:
            if not rag_retriever:
                raise HTTPException(status_code=503, detail="RAG retriever not initialized")
            rag_results = rag_retriever.retrieve(request.question, top_k=request.rag_top_k)
            rag_context = [
                {
                    "source": r.source,
                    "page": r.page,
                    "chunk_index": r.chunk_index,
                    "content": r.content,
                    "score": r.score,
                    "metadata": r.metadata
                }
                for r in rag_results
            ]

        if rag_mode == "rag_only":
            llm_answer = ""
            if request.use_llm_answer:
                llm_answer = query_engine.generate_response(
                    request.question,
                    results=[],
                    model=actual_model_used,
                    rag_context=rag_context
                ) or ""
            elapsed = round(time.time() - start_time, 2)
            return QueryResponse(
                question=request.question,
                sql="",
                answer=llm_answer,
                answer_formatted=None,
                answer_html=None,
                results=None,
                result_count=None,
                rag_context=rag_context,
                rag_mode=rag_mode,
                model_used=actual_model_used,
                use_llm_answer=request.use_llm_answer,
                elapsed_time=elapsed,
                timing=TimingInfo(total=elapsed),
                success=True,
                error=None
            )

        # Execute query with mode - pass model as parameter (thread-safe)
        sql, llm_answer, formatted_answer, html_table, raw_results, step_timing = query_engine.query_with_mode(
            request.question,
            use_llm_answer=request.use_llm_answer,
            model=actual_model_used,
            rag_context=rag_context
        )

        # Handle None values (Ollama might have failed silently)
        if sql is None:
            elapsed = round(time.time() - start_time, 2)
            return QueryResponse(
                question=request.question,
                sql="",
                answer="",
                answer_formatted=None,
                answer_html=None,
                results=None,
                result_count=None,
                rag_context=rag_context,
                rag_mode=rag_mode,
                model_used=actual_model_used,
                use_llm_answer=request.use_llm_answer,
                elapsed_time=elapsed,
                timing=TimingInfo(
                    sql_generation=step_timing.get('sql_generation'),
                    total=elapsed
                ),
                success=False,
                error="Query failed - Ollama may not be responding. Check if Ollama service is running."
            )

        if raw_results is None and rag_mode != "rag_only":
            elapsed = round(time.time() - start_time, 2)
            return QueryResponse(
                question=request.question,
                sql=sql,
                answer="",
                answer_formatted=None,
                answer_html=None,
                results=None,
                result_count=None,
                rag_context=rag_context,
                rag_mode=rag_mode,
                model_used=actual_model_used,
                use_llm_answer=request.use_llm_answer,
                elapsed_time=elapsed,
                timing=TimingInfo(
                    sql_generation=step_timing.get('sql_generation'),
                    query_execution=step_timing.get('query_execution'),
                    total=elapsed
                ),
                success=False,
                error="Query failed during execution. Check database connectivity or SQL validity."
            )

        elapsed = round(time.time() - start_time, 2)
        logger.info(f"âœ… Query successful, {len(raw_results) if raw_results else 0} results, {elapsed}s")

        return QueryResponse(
            question=request.question,
            sql=sql,
            answer=llm_answer or "",
            answer_formatted=formatted_answer,
            answer_html=html_table,
            results=raw_results,
            result_count=len(raw_results) if raw_results else 0,
            rag_context=rag_context,
            rag_mode=rag_mode,
            model_used=actual_model_used,
            use_llm_answer=request.use_llm_answer,
            elapsed_time=elapsed,
            timing=TimingInfo(
                sql_generation=step_timing.get('sql_generation'),
                query_execution=step_timing.get('query_execution'),
                formatting=step_timing.get('formatting'),
                llm_response=step_timing.get('llm_response'),
                total=elapsed
            ),
            success=True,
            error=None
        )

    except Exception as e:
        logger.error(f"âŒ Query failed: {e}")
        return QueryResponse(
            question=request.question,
            sql="",
            answer="",
            answer_formatted=None,
            answer_html=None,
            results=None,
            result_count=None,
            rag_context=None,
            rag_mode=request.rag_mode,
            model_used=request.model or (ollama_client.config.model if ollama_client else None),
            use_llm_answer=request.use_llm_answer,
            elapsed_time=round(time.time() - start_time, 2),
            success=False,
            error=str(e)
        )


@app.get("/tables", response_model=List[TableInfo], tags=["Database"])
async def get_tables():
    """
    å–å¾—è³‡æ–™è¡¨çµæ§‹è³‡è¨Š

    Returns:
        List[TableInfo]: æ‰€æœ‰è³‡æ–™è¡¨åŠå…¶æ¬„ä½è³‡è¨Š
    """
    if not db_client:
        raise HTTPException(status_code=503, detail="Database client not initialized")

    try:
        tables_info = []

        # Get table schema
        schema_query = """
        SELECT
            table_name,
            column_name,
            data_type,
            is_nullable
        FROM information_schema.columns
        WHERE table_schema = 'public'
        ORDER BY table_name, ordinal_position;
        """

        rows = db_client.execute_query(schema_query)

        # Group by table
        from collections import defaultdict
        tables_dict = defaultdict(list)

        for row in rows:
            tables_dict[row["table_name"]].append({
                "column_name": row["column_name"],
                "data_type": row["data_type"],
                "nullable": row["is_nullable"]
            })

        # Convert to TableInfo list
        for table_name, columns in tables_dict.items():
            tables_info.append(TableInfo(
                table_name=table_name,
                columns=columns
            ))

        return tables_info

    except Exception as e:
        logger.error(f"Failed to get tables: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get tables: {str(e)}")


@app.get("/demo-queries", tags=["Query"])
async def get_demo_queries():
    """
    å–å¾— Demo æŸ¥è©¢ç¯„ä¾‹

    Returns:
        List[str]: Demo æŸ¥è©¢åˆ—è¡¨
    """
    demo_queries = [
        "è«‹åˆ—å‡ºæ‰€æœ‰æœ‰åº«å­˜çš„AEDé™¤é¡«å™¨ï¼ŒåŒ…å«å“ç‰Œã€å‹è™Ÿå’Œåº«å­˜æ•¸é‡",
        "è«‹åˆ—å‡ºæ‰€æœ‰æ“”æ¶è¨­å‚™çš„å“ç‰Œã€å‹è™Ÿå’Œåº«å­˜æ•¸é‡",
        "è«‹åˆ—å‡ºå–®åƒ¹ä½æ–¼50000å…ƒçš„ç›£è¦–å™¨ï¼ŒåŒ…å«å“ç‰Œã€å‹è™Ÿå’Œåƒ¹æ ¼",
        "è«‹åˆ—å‡ºåº«å­˜æ•¸é‡ä½æ–¼10ä»¶çš„å•†å“ï¼ŒåŒ…å«ç”¢å“åç¨±ã€åˆ†é¡å’Œåº«å­˜æ•¸é‡",
        "è«‹åˆ—å‡ºæ‰€æœ‰Philipså“ç‰Œçš„ç”¢å“ï¼ŒåŒ…å«åç¨±ã€å‹è™Ÿå’Œå–®åƒ¹",
    ]

    return {
        "demo_queries": demo_queries,
        "usage": "ä½¿ç”¨ POST /query ç«¯é»åŸ·è¡Œé€™äº›æŸ¥è©¢"
    }


@app.get("/api/models", response_model=ModelsResponse, tags=["Models"])
async def get_available_models():
    """
    å–å¾—å¯ç”¨çš„ Ollama æ¨¡å‹åˆ—è¡¨

    Returns:
        ModelsResponse: å¯ç”¨æ¨¡å‹åˆ—è¡¨å’Œç•¶å‰ä½¿ç”¨çš„æ¨¡å‹
    """
    if not ollama_client:
        raise HTTPException(status_code=503, detail="Ollama client not initialized")

    try:
        models = ollama_client.get_available_models()
        current = ollama_client.config.model

        return ModelsResponse(
            models=models,
            current=current
        )
    except Exception as e:
        logger.error(f"Failed to get models: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get models: {str(e)}")


@app.post("/api/models/select", tags=["Models"])
async def select_model(request: ModelSelectRequest):
    """
    åˆ‡æ›ä½¿ç”¨çš„ Ollama æ¨¡å‹

    Args:
        request: åŒ…å«æ¨¡å‹åç¨±çš„è«‹æ±‚

    Returns:
        åˆ‡æ›çµæœ
    """
    global ollama_client, query_engine

    if not ollama_client:
        raise HTTPException(status_code=503, detail="Ollama client not initialized")

    try:
        # Check if model is available
        available_models = ollama_client.get_available_models()

        if request.model not in available_models:
            raise HTTPException(
                status_code=400,
                detail=f"Model '{request.model}' not found. Available: {available_models}"
            )

        # Update model in config
        old_model = ollama_client.config.model
        ollama_client.config.model = request.model

        # Recreate query engine with new model
        query_engine = QueryEngine(db_client, ollama_client)

        logger.info(f"ğŸ”„ Model switched from {old_model} to {request.model}")

        return {
            "success": True,
            "message": f"Model switched to {request.model}",
            "previous": old_model,
            "current": request.model
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to switch model: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to switch model: {str(e)}")


if __name__ == "__main__":
    import uvicorn

    print("ğŸš€ Starting API Server...")
    print("ğŸ“– API Documentation: http://localhost:8000/docs")
    print("ğŸ” Health Check: http://localhost:8000/health")

    uvicorn.run(
        app,
        host="0.0.0.0",  # Listen on all interfaces for remote access
        port=8000,
        log_level="info"
    )
